{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/iOS-Forks/Queryable/blob/cloud/CLIPTextModel_to_CoreML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3_XUp6s6rcF4"
   },
   "source": [
    "!pip install clip-benchmark>=1.4.0 datasets>=2.8.0 open-clip-torch>=2.20.0 timm>=0.9.5 coremltools\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#We clone MobileCLIP from: https://github.com/apple/ml-mobileclip\n",
    "!git clone https://github.com/apple/ml-mobileclip.git\n",
    "\n",
    "#Install MobileCLIP\n",
    "%cd ml-mobileclip\n",
    "!pip install -e . -q"
   ],
   "metadata": {
    "id": "xZwXLhq8rhLL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Download pretrained checkpoints\n",
    "%mkdir -p checkpoints\n",
    "!wget wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P checkpoints"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ThQZFjR6rkY5",
    "outputId": "59d41fb8-e160-465f-8862-7eefd61e86ce"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Model inference with reparamerized model\n",
    "import coremltools\n",
    "import torch\n",
    "import mobileclip\n",
    "from mobileclip.modules.common.mobileone import reparameterize_model\n",
    "from mobileclip.modules.text.tokenizer import (\n",
    "    ClipTokenizer,\n",
    ")\n",
    "from mobileclip.clip import CLIP\n",
    "from typing import Dict, Optional, Any\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# mobileclip_s0\n",
    "model_cfg = {\n",
    "    \"embed_dim\": 512,\n",
    "    \"image_cfg\": {\n",
    "        \"image_size\": 256,\n",
    "        \"model_name\": \"mci0\"\n",
    "    },\n",
    "    \"text_cfg\": {\n",
    "        \"context_length\": 77,\n",
    "        \"vocab_size\": 49408,\n",
    "        \"dim\": 512,\n",
    "        \"ffn_multiplier_per_layer\": 4.0,\n",
    "        \"n_heads_per_layer\": 8,\n",
    "        \"n_transformer_layers\": 4,\n",
    "        \"norm_layer\": \"layer_norm_fp32\",\n",
    "        \"causal_masking\": False,\n",
    "        \"model_name\": \"mct\"\n",
    "    }\n",
    "}\n",
    "\n",
    "class CLIP_encode_text(CLIP):\n",
    "    \"\"\"Class for encoding text using the text encoder from CLIP.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Dict, output_dict: bool = False, *args, **kwargs) -> None:\n",
    "        super().__init__(cfg, output_dict, *args, **kwargs)\n",
    "\n",
    "    def forward(self, text: Optional[torch.Tensor] = None) -> Any:\n",
    "        text_embeddings = (\n",
    "            self.encode_text(text, normalize=True) if text is not None else None\n",
    "        )\n",
    "        return text_embeddings\n",
    "\n",
    "\n",
    "model_text = CLIP_encode_text(cfg=model_cfg)\n",
    "model_text.eval()\n",
    "\n",
    "chkpt = torch.load(\"checkpoints/mobileclip_s0.pt\")\n",
    "model_text.load_state_dict(chkpt)  #strict=False\n",
    "\n",
    "reparameterized_model = reparameterize_model(model_text)\n",
    "reparameterized_model.eval()\n",
    "\n",
    "tokenizer = ClipTokenizer(model_cfg)\n",
    "#text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"]).to(torch.int32)\n",
    "text = tokenizer([\"a diagram\"]).to(torch.int32)\n",
    "\n",
    "text_tensor = torch.randint(0, tokenizer.get_vocab_size(), text.shape, dtype=torch.int32)\n",
    "\n",
    "traced_model = torch.jit.trace(reparameterized_model, text_tensor)\n",
    "traced_model.save(\"clip_text.pt\")\n",
    "\n",
    "input_tensor = [coremltools.TensorType(name=\"input_text\", shape=text_tensor.shape, dtype=np.int32)]\n",
    "output_tensor = [coremltools.TensorType(name=\"output_embeddings\")]\n",
    "\n",
    "ml_model = coremltools.convert(\n",
    "        model=traced_model,\n",
    "        outputs=output_tensor,\n",
    "        inputs=input_tensor,\n",
    "        convert_to=\"mlprogram\",\n",
    "        minimum_deployment_target=coremltools.target.iOS17,\n",
    "        compute_units=coremltools.ComputeUnit.ALL,\n",
    "        debug=True,\n",
    "    )\n",
    "ml_model.save(\"clip_text_s0.mlpackage\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmjOwbBhrwcx",
    "outputId": "ef12049b-a80b-44e4-b2e6-755c41064348"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Check CoreML Parameters:\n",
    "spec = ml_model.get_spec()\n",
    "print(\"model type: {}\".format(spec.WhichOneof('Type')))\n",
    "print(\"model description: {}\".format(spec.description))\n",
    "print(\"model inputs: {}\".format(spec.description.input))\n",
    "print(\"model outputs: {}\".format(spec.description.output))"
   ],
   "metadata": {
    "id": "0VJujnajsPxv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Download mlpackage\n",
    "import shutil\n",
    "# from google.colab import files\n",
    "directory_path = 'clip_text_s0.mlpackage'\n",
    "zip_path = 'clip_text_s0.mlpackage.zip'\n",
    "shutil.make_archive(directory_path, 'zip', directory_path)\n",
    "os.rename(f\"{directory_path}.zip\", zip_path)\n",
    "# files.download(zip_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-i-CXun6sCiK",
    "outputId": "a8a7f921-1763-48da-fb4f-58aca8505165"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Optional: Visualize the model\n",
    "!pip install graphviz torchview\n",
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')\n",
    "model_graph = draw_graph(reparameterized_model, input_data = text_tensor, expand_nested = True, depth = 5)\n",
    "model_graph.visual_graph"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "46_nUmZesLCk",
    "outputId": "c53e1d70-e848-4ec5-c84f-3ddb1e64299b"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
