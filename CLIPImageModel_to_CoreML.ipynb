{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/iOS-Forks/Queryable/blob/cloud/CLIPImageModel_to_CoreML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "GeS-rJ8Vhnhj"
   },
   "source": "!pip install clip-benchmark>=1.4.0 datasets>=2.8.0 open-clip-torch>=2.20.0 timm>=0.9.5 coremltools\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#We clone MobileCLIP from: https://github.com/apple/ml-mobileclip\n",
    "!git clone https://github.com/apple/ml-mobileclip.git\n",
    "\n",
    "#Install MobileCLIP\n",
    "%cd ml-mobileclip\n",
    "!pip install -e . -q"
   ],
   "metadata": {
    "id": "edfay2I4h9bi"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Download pretrained checkpoints\n",
    "%mkdir -p checkpoints\n",
    "!wget wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P checkpoints"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WsMmZj5kQ3g",
    "outputId": "aab5bf12-f981-4331-fea2-f1cd3dae58f7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Model inference with reparamerized model\n",
    "import coremltools\n",
    "import torch\n",
    "import mobileclip\n",
    "from mobileclip.modules.common.mobileone import reparameterize_model\n",
    "from mobileclip.modules.text.tokenizer import (\n",
    "    ClipTokenizer,\n",
    ")\n",
    "from mobileclip.clip import CLIP\n",
    "from typing import Dict, Optional, Any\n",
    "import json\n",
    "import os\n",
    "\n",
    "# mobileclip_s0\n",
    "model_cfg = {\n",
    "    \"embed_dim\": 512,\n",
    "    \"image_cfg\": {\n",
    "        \"image_size\": 256,\n",
    "        \"model_name\": \"mci0\"\n",
    "    },\n",
    "    \"text_cfg\": {\n",
    "        \"context_length\": 77,\n",
    "        \"vocab_size\": 49408,\n",
    "        \"dim\": 512,\n",
    "        \"ffn_multiplier_per_layer\": 4.0,\n",
    "        \"n_heads_per_layer\": 8,\n",
    "        \"n_transformer_layers\": 4,\n",
    "        \"norm_layer\": \"layer_norm_fp32\",\n",
    "        \"causal_masking\": False,\n",
    "        \"model_name\": \"mct\"\n",
    "    }\n",
    "}\n",
    "\n",
    "class CLIP_encode_image(CLIP):\n",
    "    \"\"\"Class for encoding images using the image encoder from CLIP.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Dict, output_dict: bool = False, *args, **kwargs) -> None:\n",
    "        super().__init__(cfg, output_dict, *args, **kwargs)\n",
    "\n",
    "    def forward(self, image: Optional[torch.Tensor] = None) -> Any:\n",
    "        image_embeddings = (\n",
    "            self.encode_image(image, normalize=True) if image is not None else None\n",
    "        )\n",
    "        return image_embeddings\n",
    "\n",
    "\n",
    "model_ie = CLIP_encode_image(cfg=model_cfg)\n",
    "model_ie.eval()\n",
    "\n",
    "chkpt = torch.load(\"checkpoints/mobileclip_s0.pt\")\n",
    "model_ie.load_state_dict(chkpt)\n",
    "\n",
    "reparameterized_model = reparameterize_model(model_ie)\n",
    "reparameterized_model.eval()\n",
    "\n",
    "\n",
    "image = torch.rand(1, 3, 256, 256)\n",
    "traced_model = torch.jit.trace(reparameterized_model, image)\n",
    "\n",
    "# Define the input as an image type\n",
    "input_image = coremltools.ImageType(name=\"input_image\", shape=(1, 3, 256, 256), color_layout=coremltools.colorlayout.RGB, scale=1/255.0, bias=[0, 0, 0])\n",
    "output_tensor = [coremltools.TensorType(name=\"output_embeddings\")]\n",
    "\n",
    "ml_model = coremltools.convert(\n",
    "        model=traced_model,\n",
    "        outputs=output_tensor,\n",
    "        inputs=[input_image],\n",
    "        convert_to=\"mlprogram\",\n",
    "        minimum_deployment_target=coremltools.target.iOS17,\n",
    "        compute_units=coremltools.ComputeUnit.ALL,\n",
    "        debug=True,\n",
    "    )\n",
    "ml_model.save(\"clip_mci_image_s0.mlpackage\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAE4VKBAjqQw",
    "outputId": "6452c789-a192-41a7-bd0a-727e2cc054b7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Check CoreML Parameters:\n",
    "spec = ml_model.get_spec()\n",
    "print(\"model type: {}\".format(spec.WhichOneof('Type')))\n",
    "print(\"model description: {}\".format(spec.description))\n",
    "print(\"model inputs: {}\".format(spec.description.input))\n",
    "print(\"model outputs: {}\".format(spec.description.output))"
   ],
   "metadata": {
    "id": "tkG9QhmSsWfP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Download mlpackage\n",
    "import shutil\n",
    "# from google.colab import files\n",
    "directory_path = 'clip_mci_image_s0.mlpackage'\n",
    "zip_path = 'clip_mci_image_s0.mlpackage.zip'\n",
    "shutil.make_archive(directory_path, 'zip', directory_path)\n",
    "os.rename(f\"{directory_path}.zip\", zip_path)\n",
    "# files.download(zip_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "MdWzx9HEmhBR",
    "outputId": "1d217c06-5be9-4809-c5d4-7da58be9dbb9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install graphviz==0.20.3 torchview==0.2.6\n",
    "#Optional: Visualize the model\n",
    "# !pip graphviz torchview\n",
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')\n",
    "model_graph = draw_graph(reparameterized_model, input_data = image, expand_nested = True, depth = 5)\n",
    "model_graph.visual_graph"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OiVjROoqo_Za",
    "outputId": "144b937a-25ba-47c1-9104-4a33b5b90e7e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
